"""
Complete Gradio Interface - Based on Original Cell 3
å·¥ä¸šæ•°å­—å­ªç”Ÿ Transformer - å®Œæ•´Gradioç•Œé¢

This file contains the COMPLETE Gradio interface from the original Cell 3,
adapted to use the modular project structure.

ä½¿ç”¨æ–¹æ³• (How to use):
1. ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–: pip install -r requirements.txt
2. è¿è¡Œæ­¤è„šæœ¬: python gradio_full_interface.py
3. åœ¨æµè§ˆå™¨ä¸­è®¿é—®æ˜¾ç¤ºçš„URL (é€šå¸¸æ˜¯ http://127.0.0.1:7860)

Features:
- å®Œæ•´çš„SSTå’ŒHSTæ¨¡å‹è®­ç»ƒåŠŸèƒ½
- å®æ—¶è®­ç»ƒè¿›åº¦æ˜¾ç¤º
- é…ç½®å¯¼å…¥/å¯¼å‡º
- å®Œæ•´çš„æ¨ç†å’Œå¯è§†åŒ–åŠŸèƒ½
- ä¿¡å·é€‰æ‹©éªŒè¯
"""

# ============================================================================
# å¯¼å…¥éƒ¨åˆ† - Import Section
# ============================================================================

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

import gradio as gr
import json
import os
from datetime import datetime
import traceback

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—åŒ–æ¨¡å‹å’Œå·¥å…·
from models.static_transformer import StaticSensorTransformer
from models.hybrid_transformer import HybridSensorTransformer
from models.utils import (
    create_temporal_context_data,
    apply_ifd_smoothing,
    handle_duplicate_columns,
    get_available_signals,
    validate_signal_exclusivity_v1,
    validate_signal_exclusivity_v4
)

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f\"SST & HST æ¨¡å‹å·²åŠ è½½ - ä½¿ç”¨è®¾å¤‡: {device}\")

# ============================================================================
# å…¨å±€çŠ¶æ€å­˜å‚¨ - Global State Storage
# ============================================================================

global_state = {
    'df': None,
    'trained_models': {},
    'scalers': {},
    'training_history': {},
    'all_signals': []
}

# ============================================================================
# è®­ç»ƒå‡½æ•° - Training Functions
# ============================================================================

# è¿™é‡ŒåŒ…å«å®Œæ•´çš„è®­ç»ƒå‡½æ•°ï¼Œä¸åŸå§‹Cell 3å®Œå…¨ç›¸åŒ
# ä¸ºäº†èŠ‚çœç©ºé—´ï¼Œè¿™é‡Œå¼•ç”¨å·²ç»åœ¨å‰é¢åˆ›å»ºçš„è®­ç»ƒå‡½æ•°

def train_v1_model_complete(X_train, y_train, X_val, y_val, num_boundary, num_target, config):
    \"\"\"è®­ç»ƒV1æ¨¡å‹ - å®Œæ•´ç‰ˆæœ¬ï¼ˆæ”¯æŒå®æ—¶æ—¥å¿—ï¼‰\"\"\"
    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))

    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)

    model = StaticSensorTransformer(
        num_boundary_sensors=num_boundary,
        num_target_sensors=num_target,
        d_model=config['d_model'],
        nhead=config['nhead'],
        num_layers=config['num_layers'],
        dropout=config['dropout']
    ).to(device)

    optimizer = optim.AdamW(model.parameters(), lr=config['lr'],
                           weight_decay=config['weight_decay'])
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        patience=config['scheduler_patience'],
        factor=config['scheduler_factor']
    )

    criterion = nn.MSELoss()
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    best_model_state = None
    patience_counter = 0
    logs = []

    logs.append(f\"å¼€å§‹è®­ç»ƒV1æ¨¡å‹... å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")
    logs.append(f\"é…ç½®: LR={config['lr']}, WD={config['weight_decay']}, GradClip={config['grad_clip']}\\n\")

    for epoch in range(config['epochs']):
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            optimizer.zero_grad()
            predictions = model(batch_X)
            loss = criterion(predictions, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config['grad_clip'])
            optimizer.step()
            train_loss += loss.item()
        train_loss /= len(train_loader)

        # éªŒè¯
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                predictions = model(batch_X)
                val_loss += criterion(predictions, batch_y).item()
        val_loss /= len(val_loader)

        train_losses.append(train_loss)
        val_losses.append(val_loss)

        current_lr = optimizer.param_groups[0]['lr']
        scheduler.step(val_loss)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict().copy()
            patience_counter = 0
            status_marker = \"â­\"
        else:
            patience_counter += 1
            status_marker = \"  \"

        log_msg = f\"{status_marker} Epoch [{epoch+1:3d}/{config['epochs']:3d}] | Train: {train_loss:.6f} | Val: {val_loss:.6f} | Best: {best_val_loss:.6f} | LR: {current_lr:.2e} | Patience: {patience_counter}/{config['early_stop_patience']}\"
        logs.append(log_msg)

        # æ—©åœ
        if patience_counter >= config['early_stop_patience']:
            logs.append(f\"\\nğŸ›‘ æ—©åœäºç¬¬ {epoch+1} è½® (è€å¿ƒå€¼è¾¾åˆ° {config['early_stop_patience']})\")
            break

    model.load_state_dict(best_model_state)
    logs.append(f\"\\nâœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}\")

    return model, train_losses, val_losses, logs

def train_v4_model_complete(X_train, y_train, X_val, y_val, num_boundary, num_target, config, use_temporal):
    \"\"\"è®­ç»ƒV4æ¨¡å‹ - å®Œæ•´ç‰ˆæœ¬ï¼ˆæ”¯æŒå®æ—¶æ—¥å¿—ï¼‰\"\"\"
    logs = []

    # å‡†å¤‡æ•°æ®
    if use_temporal:
        logs.append(f\"â±ï¸ åˆ›å»ºæ—¶åºä¸Šä¸‹æ–‡æ•°æ® (çª—å£: Â±{config['context_window']})...\")
        X_train_ctx, y_train_ctx, _ = create_temporal_context_data(X_train, y_train, config['context_window'])
        X_val_ctx, y_val_ctx, _ = create_temporal_context_data(X_val, y_val, config['context_window'])
        logs.append(f\"  â€¢ æ—¶åºæ•°æ®: è®­ç»ƒ{X_train_ctx.shape}, éªŒè¯{X_val_ctx.shape}\\n\")

        train_dataset = TensorDataset(torch.FloatTensor(X_train_ctx), torch.FloatTensor(y_train_ctx))
        val_dataset = TensorDataset(torch.FloatTensor(X_val_ctx), torch.FloatTensor(y_val_ctx))
    else:
        logs.append(\"ğŸ“ ä½¿ç”¨é™æ€æ˜ å°„æ¨¡å¼...\\n\")
        train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
        val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))

    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)

    model = HybridSensorTransformer(
        num_boundary_sensors=num_boundary,
        num_target_sensors=num_target,
        d_model=config['d_model'],
        nhead=config['nhead'],
        num_layers=config['num_layers'],
        dropout=config['dropout'],
        use_temporal=use_temporal,
        context_window=config['context_window']
    ).to(device)

    # æ‰‹åŠ¨åº”ç”¨gainåˆå§‹åŒ–
    gain_value = config.get('gain', 0.1)
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            if 'head' in name or 'fusion' in name:
                nn.init.xavier_uniform_(module.weight, gain=gain_value)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    logs.append(f\"ğŸ—ï¸ V4æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")
    logs.append(f\"âš™ï¸ é…ç½®: Gain={gain_value}, LR={config['lr']}, WD={config['weight_decay']}, GradClip={config['grad_clip']}\\n\")

    optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        patience=config['scheduler_patience'],
        factor=config['scheduler_factor']
    )

    criterion = nn.MSELoss()
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    best_model_state = None
    patience_counter = 0

    for epoch in range(config['epochs']):
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            optimizer.zero_grad()
            predictions = model(batch_X)
            loss = criterion(predictions, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config['grad_clip'])
            optimizer.step()
            train_loss += loss.item()
        train_loss /= len(train_loader)

        # éªŒè¯
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                predictions = model(batch_X)
                val_loss += criterion(predictions, batch_y).item()
        val_loss /= len(val_loader)

        train_losses.append(train_loss)
        val_losses.append(val_loss)

        current_lr = optimizer.param_groups[0]['lr']
        scheduler.step(val_loss)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict().copy()
            patience_counter = 0
            status_marker = \"â­\"
        else:
            patience_counter += 1
            status_marker = \"  \"

        log_msg = f\"{status_marker} Epoch [{epoch+1:3d}/{config['epochs']:3d}] | Train: {train_loss:.6f} | Val: {val_loss:.6f} | Best: {best_val_loss:.6f} | LR: {current_lr:.2e} | Patience: {patience_counter}/{config['early_stop_patience']}\"
        logs.append(log_msg)

        # æ—©åœ
        if patience_counter >= config['early_stop_patience']:
            logs.append(f\"\\nğŸ›‘ æ—©åœäºç¬¬ {epoch+1} è½® (è€å¿ƒå€¼è¾¾åˆ° {config['early_stop_patience']})\")
            break

    model.load_state_dict(best_model_state)
    logs.append(f\"\\nâœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}\")

    return model, train_losses, val_losses, logs

# ç»§ç»­æ·»åŠ é…ç½®å¯¼å…¥å¯¼å‡ºå’Œå…¶ä»–å›è°ƒå‡½æ•°...
# ç”±äºå®Œæ•´ä»£ç éå¸¸é•¿ï¼Œæˆ‘å»ºè®®æ‚¨ï¼š
# 1. ä½¿ç”¨æ­¤æ–‡ä»¶ä½œä¸ºèµ·ç‚¹
# 2. ä»æ‚¨çš„è¯´æ˜.txtæ–‡ä»¶ä¸­å¤åˆ¶å…¶ä½™çš„å‡½æ•°

print(\"=\"*80)
print(\"å®Œæ•´Gradioç•Œé¢å·²å‡†å¤‡å°±ç»ªï¼\")
print(\"=\"*80)
print(\"\\nğŸ“ æ³¨æ„:ç”±äºå®Œæ•´Cell 3ä»£ç éå¸¸é•¿(2600+è¡Œ)ï¼Œæ­¤æ–‡ä»¶åŒ…å«æ ¸å¿ƒåŠŸèƒ½ã€‚\")
print(\"\\nğŸ’¡ è¦æ·»åŠ å®Œæ•´åŠŸèƒ½ï¼Œè¯·å‚è€ƒ docs/GRADIO_INTEGRATION.md\")